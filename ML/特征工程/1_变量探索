#完整数据信息展示
def resumetable(df):
    print(f"Dataset Shape: {df.shape}")
    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])
    summary = summary.reset_index()
    summary['Name'] = summary['index']
    summary = summary[['Name','dtypes']]
    summary['Missing'] = df.isnull().sum().values    
    summary['Uniques'] = df.nunique().values
    summary['First Value'] = df.loc[0].values
    summary['Second Value'] = df.loc[1].values
    summary['Third Value'] = df.loc[2].values

    for name in summary['Name'].value_counts().index:
        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) 
    return summary
# #### 数据格式trainX.info()
trainX.columns
testX.info()
trainy.info()
resumetable(trainX)


#每个数字特征得分布可视化distplot
f = pd.melt(data_train, value_vars=numerical_serial_fea)
g = sns.FacetGrid(f, col="variable",  col_wrap=2, sharex=False, sharey=False)
g = g.map(sns.distplot, "value")


#kde特征分布图
for column in data.columns[0:-1]:
    g = sns.kdeplot(data[column][(data["oringin"] == "train")], color="Red", shade = True)
    g = sns.kdeplot(data[column][(data["oringin"] == "test")], ax =g, color="Green", shade= True)
    g.set_xlabel(column)
    g.set_ylabel("Frequency")
    g = g.legend(["train","test"])
    plt.show()
    
#查看类别型变量在不同y值上的分布
train_loan_fr = data_train.loc[data_train['isDefault'] == 1]
train_loan_nofr = data_train.loc[data_train['isDefault'] == 0]

fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 8))
train_loan_fr.groupby('grade')['grade'].count().plot(kind='barh', ax=ax1, title='Count of grade fraud')
train_loan_nofr.groupby('grade')['grade'].count().plot(kind='barh', ax=ax2, title='Count of grade non-fraud')
train_loan_fr.groupby('employmentLength')['employmentLength'].count().plot(kind='barh', ax=ax3, title='Count of employmentLength fraud')
train_loan_nofr.groupby('employmentLength')['employmentLength'].count().plot(kind='barh', ax=ax4, title='Count of employmentLength non-fraud')
plt.show()

#查看连续型变量在不同y值上的分布
fig, ((ax1, ax2)) = plt.subplots(1, 2, figsize=(15, 6))
data_train.loc[data_train['isDefault'] == 1] \
    ['loanAmnt']\
    .plot(kind='hist',
          bins=100,
          title=' Amt - Fraud',
          color='r',
          xlim=(-3, 10),
         ax= ax1)
data_train.loc[data_train['isDefault'] == 0] \
    ['loanAmnt']\
    .plot(kind='hist',
          bins=100,
          title='Amt - Not Fraud',
          color='b',
          xlim=(-3, 10),
         ax=ax2)
         
###缺失值填充，填充后带入模型验证哪种填充方式效果更好###

# 全为缺失值删除
train_data.dropna(how='all') 

#利用均值/众数/中位数填充缺失值        
def impute_NA_with_avg(data,strategy='mean',NA_col=[]):
    """
    replacing the NA with mean/median/most frequent values of that variable. 
    Note it should only be performed over training set and then propagated to test set.
    """
    
    data_copy = data.copy(deep=True)
    for i in NA_col:
        if data_copy[i].isnull().sum()>0:
            if strategy=='mean':
                data_copy[i+'_impute_mean'] = data_copy[i].fillna(data[i].mean())
            elif strategy=='median':
                data_copy[i+'_impute_median'] = data_copy[i].fillna(data[i].median())
            elif strategy=='mode':
                data_copy[i+'_impute_mode'] = data_copy[i].fillna(data[i].mode()[0])
        else:
            warn("Column %s has no missing" % i)
    return data_copy
# 缺失值进行填充
train_data = impute_NA_with_avg(train_data, column_headers) 
df_predict = impute_NA_with_avg(df_predict, column_headers) 
train_data.fillna(0, inplace = True) 
df_predict.fillna(0, inplace = True) 
  
#KNN计算缺失值
k=3 
imputer = KNNImputer(n_neighbors=k)
imputed = imputer.fit_transform(data)
#确定k值
def optimize_k(data, target):
    errors = []
    for k in range(1, 20, 2):
        imputer = KNNImputer(n_neighbors=k)
        imputed = imputer.fit_transform(data)
        df_imputed = pd.DataFrame(imputed, columns=df.columns)
        
        X = df_imputed.drop(target, axis=1)
        y = df_imputed[target]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        model = RandomForestRegressor() 
        model.fit(X_train, y_train)
        preds = model.predict(X_test)
        error = rmse(y_test, preds) #评估指标根据实际情况自定义
        errors.append({'K': k, 'RMSE': error})
    return errors

#随机森林计算缺失值
##填补一个特征，先将其他特征值的缺失值用0代替，这样每次循环一次，有缺失值的特征便会减少一个
from sklearn.impute import SimpleImputer  # 填充缺失值的类
# 找出缺失值从小到大对应的索引值
sortindex = np.argsort(X_missing_reg.isnull().sum(axis=0)).values
for i in sortindex:

    # 构建新的特征矩阵和新标签
    df = X_missing_reg   # 所有的操作都在df上进行，只是最后得到的填充值作用在X_missing_reg上面
    fillc = df.iloc[:, i]   # 某个需要填充的列，索引为i
    
    # 没有被选中填充（!=）的特征与原始标签的连接起来；df就是新特征矩阵
    df = pd.concat([df.iloc[:, df.columns != i], pd.DataFrame(y_full)], axis=1)

    # 新的特征矩阵df中，对含有缺失值的列，进行0的填补
    # 检查是否有0 pd.DataFrame(df_0).isnull().sum()
    df_0 = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0).fit_transform(df)

    # 找出训练集和测试集
    ytrain = fillc[fillc.notnull()]  # 被选中填充的特征矩阵T中的非空值
    ytest = fillc[fillc.isnull()]  # 被选中填充的特征矩阵T中的空值
    Xtrain = df_0[ytrain.index, :]  # 新特征矩阵上，被选出来要填充的特征的非空值对应的记录
    Xtest = df_0[ytest.index, :]   # 空值对应的记录

    # 随机森林填充缺失值
    rfc = RandomForestRegressor(n_estimators=100)
    rfc = rfc.fit(Xtrain, ytrain)
    y_predict = rfc.predict(Xtest)  # predict接口预测得到的结果就是用来填充空值的那些值

    # 将填补好的特征返回到我们的原始特征矩阵中
    X_missing_reg.loc[X_missing_reg.iloc[:, i].isnull(), i] = y_predict

###异常值处理
#3sigma
def find_outliers_by_3segama(data,fea):
    data_std = np.std(data[fea])
    data_mean = np.mean(data[fea])
    outliers_cut_off = data_std * 3
    lower_rule = data_mean - outliers_cut_off
    upper_rule = data_mean + outliers_cut_off
    data[fea+'_outliers'] = data[fea].apply(lambda x:str('异常值') if x > upper_rule or x < lower_rule else '正常值')
    return data
    
#分位数
from collections import Counter
def detect_outliers(df,n,features):
    """
    Takes a dataframe df of features and returns a list of the indices
    corresponding to the observations containing more than n outliers according
    to the Tukey method.
    """
    outlier_indices = []
    
    # iterate over features(columns)
    for col in features:
        # 1st quartile (25%)
        Q1 = np.percentile(df[col], 25)
        # 3rd quartile (75%)
        Q3 = np.percentile(df[col],75)
        # Interquartile range (IQR)
        IQR = Q3 - Q1
        
        # outlier step
        outlier_step = 1.5 * IQR
        
        # Determine a list of indices of outliers for feature col
        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index
        
        # append the found outlier indices for col to the list of outlier indices 
        outlier_indices.extend(outlier_list_col)
        
    # select observations containing more than 2 outliers
    outlier_indices = Counter(outlier_indices)        
    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )
    
    return multiple_outliers   

# detect outliers from Age, SibSp , Parch and Fare
Outliers_to_drop = detect_outliers(train,5,(data.columns.tolist())[1:-1])
len(Outliers_to_drop)
train = train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)

